\documentclass{article}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{float}       % For [H] table placement
\usepackage{titlesec}    % For compact section spacing

% --- Reduce white space ---
\usepackage[margin=2.5cm]{geometry}
\setlength{\floatsep}{4pt plus 1pt minus 1pt}
\setlength{\textfloatsep}{6pt plus 1pt minus 1pt}
\setlength{\intextsep}{4pt plus 1pt minus 1pt}
\setlength{\abovecaptionskip}{4pt}
\setlength{\belowcaptionskip}{2pt}
\setlength{\parskip}{2pt plus 1pt}
\titlespacing*{\section}{0pt}{8pt}{4pt}
\titlespacing*{\subsection}{0pt}{6pt}{2pt}

\begin{document}

\section{Framework Evaluation and Comparative Analysis}
Three open-source RAG frameworks were evaluated: \textbf{Haystack} (retrieval-first), \textbf{LangChain} (agent-first), and \textbf{LlamaIndex} (index-first). The criteria included architectural flexibility, local deployment, multilingual support (English/Arabic), retrieval strategies, configurability, and production-readiness.

\subsection{High-Level Positioning}
\begin{table}[H]
\centering
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Framework} & \textbf{Philosophy} & \textbf{Best For} & \textbf{Architecture Style} \\ \midrule
Haystack   & Retrieval-first orchestration & Production RAG          & Explicit pipeline graph \\
LangChain  & Agent-first composition       & Tool-heavy applications & Chain/agent abstraction \\
LlamaIndex & Index-first knowledge system  & Rapid RAG prototyping   & Index abstraction layer \\ \bottomrule
\end{tabular}
\caption{High-level positioning of the evaluated frameworks.}
\label{tab:positioning}
\end{table}

\subsection{Core RAG Capabilities, Evaluation, and Multimodal Support}
Table~\ref{tab:capabilities} consolidates retrieval features, production readiness, and multimodal support.
\begin{table}[H]
\centering
\small
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Feature} & \textbf{Haystack} & \textbf{LangChain} & \textbf{LlamaIndex} \\ \midrule
\multicolumn{4}{@{}l}{\textit{Retrieval \& Pipeline}} \\
Dense retrieval      & Native     & Native               & Native              \\
Sparse (BM25)        & Strong     & Basic                & Limited             \\
Hybrid search        & Built-in   & Manual configuration & Custom              \\
Rerankers            & Built-in   & Manual integration   & Add-on              \\
Router pipelines     & Native     & Manual logic         & Query engine routing \\
Multi-step RAG       & Explicit   & Agent-based          & Query planners      \\
Streaming            & Supported  & Supported            & Supported           \\
Async pipelines      & Supported  & Partial              & Partial             \\
Metadata filtering   & Strong     & Supported            & Supported           \\ \midrule
\multicolumn{4}{@{}l}{\textit{Evaluation \& Production}} \\
Retrieval metrics    & Supported  & Not native           & Limited             \\
Faithfulness scoring & Supported  & Not native           & Limited             \\
Groundedness tools   & Supported  & Not native           & Limited             \\
Logging and tracing  & Built-in   & LangSmith (cloud)    & Basic               \\
Production stability & High       & Rapid API evolution   & Moderate            \\ \midrule
\multicolumn{4}{@{}l}{\textit{Multimodal \& Structured Data}} \\
Image embeddings       & Supported          & Stronger ecosystem & Early-stage        \\
Vision-language models & Manual integration & Supported          & Limited            \\
Table QA               & Custom pipelines   & Tool-based         & Structured indexes \\
CSV ingestion          & Supported          & Supported          & Supported          \\ \bottomrule
\end{tabular}
\caption{Core capabilities, evaluation, and multimodal support.}
\label{tab:capabilities}
\end{table}

\subsection{Infrastructure: Document Stores, Vector Databases, and LLM Backends}
Table~\ref{tab:infrastructure} combines vector database compatibility and LLM backend support.
\begin{table}[H]
\centering
\small
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Backend} & \textbf{Haystack} & \textbf{LangChain} & \textbf{LlamaIndex} \\ \midrule
\multicolumn{4}{@{}l}{\textit{Vector / Document Stores}} \\
Qdrant                 & Native    & Supported            & Supported \\
Elasticsearch          & Supported & Supported            & Limited   \\
OpenSearch             & Supported & Limited              & Limited   \\
Milvus                 & Supported & Supported            & Supported \\
Pinecone               & Supported & Supported            & Supported \\
InMemory store         & Native    & Limited              & Limited   \\
Hybrid vector + sparse & Strong    & Manual configuration & Limited   \\ \midrule
\multicolumn{4}{@{}l}{\textit{LLM Backends (local + cloud)}} \\
vLLM                             & OpenAI-compatible & Supported & Supported \\
Ollama                           & Native            & Supported & Supported \\
HuggingFace Transformers (local) & Basic wrapper     & Supported & Supported \\
Text Generation Inference (TGI)  & Supported         & Supported & Supported \\
OpenAI API                       & Supported         & Supported & Supported \\ \bottomrule
\end{tabular}
\caption{Document store, vector database, and LLM backend compatibility.}
\label{tab:infrastructure}
\end{table}

\subsection{Pipeline Configurability}
\begin{table}[H]
\centering
\small
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Requirement} & \textbf{Haystack} & \textbf{LangChain} & \textbf{LlamaIndex} \\ \midrule
Configurable chunking      & Component-level   & Preprocessing stage  & Index configuration    \\
Dynamic retriever swapping & Supported         & Manual chain rebuild & Index rebuild required \\
Toggle reranker on/off     & Easy              & Manual               & Manual                 \\
Switch LLM backend         & Supported         & Supported            & Supported              \\
Expose parameters via API  & Clean integration & More manual setup    & Moderate               \\
Visual pipeline graph      & Available         & Not available        & Not available          \\ \bottomrule
\end{tabular}
\caption{Pipeline configurability for runtime customization.}
\label{tab:configurability}
\end{table}

\section{Retrieval Capabilities}

\subsection{Dense Retrieval}
Embeds documents and queries into a shared vector space; retrieves by cosine/dot-product similarity. Captures semantic meaning across English and Arabic, well-suited for natural language questions.
\begin{center}
\texttt{Text $\rightarrow$ Embedder $\rightarrow$ Vector Store $\leftarrow$ Query Embedder $\leftarrow$ Query}
\end{center}

\subsection{Sparse Retrieval (BM25)}
Keyword-based scoring using term frequency. Highly effective for exact-match queries, formal Arabic text, legal documents, and structured content. No embedding model required.

\subsection{Hybrid Retrieval}
Fuses dense and sparse scores into a single ranking. Significantly improves retrieval robustness and is highly recommended for enterprise RAG.
\begin{center}
\texttt{BM25 Score + Dense Score $\rightarrow$ Fused Ranking $\rightarrow$ Top-K Documents}
\end{center}

\subsection{Reranking (Cross-Encoder)}
A second-stage model re-evaluates retrieved documents before passing them to the LLM. Reduces hallucinations and boosts answer quality at the cost of slight additional latency.
\begin{center}
\texttt{Retriever $\rightarrow$ Reranker $\rightarrow$ LLM}
\end{center}

\section{Document Processing}

\subsection{Chunking Strategies}
Splitting documents into retrievable pieces directly affects retrieval quality---too small loses context, too large introduces noise. Haystack supports multiple strategies including smart and context-aware approaches. Chunk size and overlap are exposed as configurable API parameters. Full reference: \\\url{https://docs.haystack.deepset.ai/docs/documentsplitter}

\subsection{Metadata Filtering}
Filters the search space before retrieval using document attributes (e.g., language, date, category). Metadata is attached at indexing time via components like \texttt{DocumentLanguageClassifier}, and \texttt{MetadataRouter} routes documents accordingly.

\subsection{File Ingestion}
Haystack supports heterogeneous input formats: PDF, DOCX, Markdown, HTML, JSON, and CSV.

\end{document}
